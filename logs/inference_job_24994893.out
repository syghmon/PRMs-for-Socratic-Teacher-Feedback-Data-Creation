Loading model '/cluster/scratch/sstorf/DeepSeek-R1-Distill-Qwen-7B'... (This may take a while for large models)
INFO 03-03 09:56:43 __init__.py:207] Automatically detected platform cuda.
WARNING 03-03 09:56:43 config.py:2448] Casting torch.bfloat16 to torch.float16.
INFO 03-03 09:58:00 config.py:549] This model supports multiple tasks: {'score', 'classify', 'generate', 'embed', 'reward'}. Defaulting to 'generate'.
WARNING 03-03 09:58:00 arg_utils.py:1187] Chunked prefill is enabled by default for models with max_model_len > 32K. Currently, chunked prefill might not work with some features or models. If you encounter any issues, please disable chunked prefill by setting --enable-chunked-prefill=False.
INFO 03-03 09:58:00 config.py:1555] Chunked prefill is enabled with max_num_batched_tokens=2048.
INFO 03-03 09:58:00 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.3) with config: model='/cluster/scratch/sstorf/DeepSeek-R1-Distill-Qwen-7B', speculative_config=None, tokenizer='/cluster/scratch/sstorf/DeepSeek-R1-Distill-Qwen-7B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/cluster/scratch/sstorf/DeepSeek-R1-Distill-Qwen-7B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 03-03 09:58:06 cuda.py:178] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.
INFO 03-03 09:58:06 cuda.py:226] Using XFormers backend.
INFO 03-03 09:58:13 model_runner.py:1110] Starting to load model /cluster/scratch/sstorf/DeepSeek-R1-Distill-Qwen-7B...
Error during model loading or inference: CUDA out of memory. Tried to allocate 130.00 MiB. GPU 0 has a total capacity of 10.75 GiB of which 10.69 MiB is free. Including non-PyTorch memory, this process has 10.73 GiB memory in use. Of the allocated memory 10.51 GiB is allocated by PyTorch, and 46.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
